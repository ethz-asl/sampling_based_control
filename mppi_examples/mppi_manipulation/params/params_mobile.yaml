solver:
  rollouts: 40
  lambda: 0.0001 #0.001
  h: 10.0
  step_size: 0.015
  horizon: 1.0

  cost_ratio: 0.2
  caching_factor: 0.3
  learned_rollout_ratio: 0.0  # percentage of rollouts that is informed by the learned expert
  filtering: true
  substeps: 1

  gradient_step_size: 1.0
  momentum_step_size: 0.0

  filter_type: 1
  filter_order: 1
  filter_window: 10

  filters_type: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
  filters_order: [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1]
  filters_window: [50, 50, 50, 10, 10, 10, 10, 10, 10, 10, 10]

  bound_input: true
  u_min: [-0.2, -0.2, -0.2, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]
  u_max: [0.2, 0.2, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

  adaptive_sampling: false
  input_variance: [0.05, 0.05, 0.05, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]

  use_tree_search: false         # if true tree structure is used
  pruning_threshold: 0.04       # controls the structure of the tree
  # expert_types: {NORM, IMP}
  expert_weights: [ 0.1, 0.5 ]  # controls the weights of the different experts

  discount_factor: 1.0
  verbose: false
  threads: 8
  debug_print: false

  logging: true

policy_update_rate: 0.0  # execute as fast as possible
reference_update_rate: 10.0
publish_ros: true
ros_publish_rate: 10.0

sequential: false
static_optimization: false
sim_dt: 0.015

initial_configuration: [0.0, 0.0, 0.0, 0.0, -0.52, 0.0, -1.785, 0.0, 1.10, 0.69, 0.04, 0.04]

base_gains:
  kp: [0.0, 0.0, 0.0]
  kd: [1000.0, 1000.0, 1000.0]

arm_gains: 
  kp: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
  kd: [10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]

gripper_gains:
  kp: [100.0, 100.0]
  kd: [50.0, 50.0]
